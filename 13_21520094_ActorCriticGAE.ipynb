{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9wMdTOc+a5LXmTwwXL2/j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#TÓM TẮT CÁC BƯỚC CHÍNH CỦA THUẬT TOÁN ACTOR-CRITIC VỚI GAE\n","# 1. **Khởi tạo môi trường và siêu tham số**:\n","#    - Tạo môi trường \"CartPole-v1\" và định nghĩa các tham số cần thiết như gamma, lambda, số bước tối đa, số tập tối đa, v.v.\n","# 2. **Xây dựng mô hình Actor-Critic**:\n","#    - Mô hình gồm 2 phần:\n","#      - **Actor**: Dự đoán xác suất chọn các hành động.\n","#      - **Critic**: Dự đoán giá trị của trạng thái hiện tại.\n","# 3. **Huấn luyện mô hình**:\n","#    - Trong mỗi tập (episode), mô hình:    \n","*   Chọn hành động dựa trên chính sách (policy).\n","*   Thực hiện hành động trong môi trường và lưu thông tin phần thưởng, giá trị trạng thái.    \n","#    - Tính toán Advantage và Returns bằng GAE (Generalized Advantage Estimation).\n","#    - Tính toán mất mát Actor, Critic, và Entropy để cập nhật trọng số mô hình.\n","# 4. **Điều chỉnh độ nhiễu Entropy**:\n","#    - Giảm dần Entropy Beta dựa trên phần thưởng trung bình để chuyển từ giai đoạn khám phá sang khai thác.\n","# 5. **Theo dõi tiến trình và kết thúc huấn luyện**:\n","#    - Dừng huấn luyện khi phần thưởng trung bình đạt ngưỡng hoặc đạt số tập tối đa."],"metadata":{"id":"-YFG9nSILOGR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"erlvGMuIqRwW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733190516359,"user_tz":-420,"elapsed":15360,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"93c5be69-ca51-4ed8-82b5-b2671e3a7674"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n","  from jax import xla_computation as _xla_computation\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Thiết lập backend sử dụng TensorFlow\n","import gym  # Thư viện mô phỏng môi trường\n","import numpy as np  # Xử lý số liệu\n","import keras\n","from keras import ops  # Công cụ toán học cho TensorFlow\n","from keras import layers  # Định nghĩa các lớp mạng nơ-ron\n","import tensorflow as tf  # Framework học sâu"]},{"cell_type":"code","source":["# ==================== BƯỚC 1: KHỞI TẠO MÔI TRƯỜNG VÀ CẤU HÌNH ====================\n","# Tạo môi trường CartPole-v1\n","env = gym.make(\"CartPole-v1\")\n","# Định nghĩa các siêu tham số\n","num_inputs = 4  # Số lượng đầu vào (trạng thái)\n","num_actions = 2  # Số hành động có thể thực hiện\n","num_hidden1 = 256  # Số lượng nơ-ron trong lớp ẩn đầu tiên\n","num_hidden2 = 128  # Số lượng nơ-ron trong lớp ẩn thứ hai\n","gamma = 0.97  # Hệ số chiết khấu\n","lambda_ = 0.95  # Hệ số GAE\n","max_steps_per_episode = 500  # Số bước tối đa mỗi tập\n","max_episodes = 1000  # Số tập tối đa\n","initial_entropy_beta = 0.01  # Hệ số entropy ban đầu\n","learning_rate = 0.001  # Tốc độ học"],"metadata":{"id":"tjrD_5UWqdMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================== BƯỚC 2: XÂY DỰNG MÔ HÌNH ACTOR-CRITIC ====================\n","# Định nghĩa các lớp và đầu ra cho Actor và Critic\n","inputs = layers.Input(shape=(num_inputs,))  # Đầu vào là trạng thái\n","common = layers.Dense(num_hidden1, activation=\"relu\")(inputs)  # Lớp ẩn 1 với hàm kích hoạt ReLU\n","common = layers.LayerNormalization()(common)  # Chuẩn hóa đầu ra lớp ẩn 1\n","common = layers.Dense(num_hidden2, activation=\"relu\")(common)  # Lớp ẩn 2 với hàm kích hoạt ReLU\n","action = layers.Dense(num_actions, activation=\"softmax\")(common)  # Lớp đầu ra cho Actor (xác suất hành động)\n","critic = layers.Dense(1)(common)  # Lớp đầu ra cho Critic (giá trị trạng thái)\n","# Kết hợp các đầu ra để tạo mô hình Actor-Critic\n","model = tf.keras.Model(inputs=inputs, outputs=[action, critic])\n","\n","# Định nghĩa hàm tối ưu hóa và hàm mất mát\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Sử dụng Adam Optimizer\n","huber_loss = tf.keras.losses.Huber()  # Hàm mất mát Huber\n"],"metadata":{"id":"XnKXgCQvqsI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hàm tính toán GAE (Generalized Advantage Estimation) và Returns\n","def compute_gae_and_returns(rewards, values, gamma=0.97, lambda_=0.95):\n","    advantages = []  # Lưu trữ Advantage\n","    gae = 0  # Giá trị GAE ban đầu\n","    returns = []  # Lưu trữ Returns\n","    # Duyệt ngược qua danh sách phần thưởng để tính toán GAE\n","    for t in reversed(range(len(rewards))):\n","        # Tính delta (phần sai lệch)\n","        delta = rewards[t] + gamma * (values[t + 1] if t + 1 < len(values) else 0) - values[t]\n","        gae = delta + gamma * lambda_ * gae  # Tính giá trị GAE\n","        advantages.insert(0, gae)  # Thêm Advantage vào danh sách\n","        returns.insert(0, gae + values[t])  # Tính Return\n","    return np.array(advantages), np.array(returns)  # Trả về Advantage và Returns"],"metadata":{"id":"q6sfRNl_qvhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==================== BƯỚC 3: HUẤN LUYỆN MÔ HÌNH ====================\n","running_reward = 0  # Theo dõi phần thưởng trung bình\n","entropy_beta = initial_entropy_beta  # Hệ số entropy ban đầu\n","\n","# Vòng lặp huấn luyện qua các tập (episodes)\n","for episode_count in range(1, max_episodes + 1):\n","    state = env.reset()  # Khởi tạo trạng thái\n","    episode_reward = 0  # Tổng phần thưởng mỗi tập\n","    action_probs_history = []  # Lưu lịch sử xác suất hành động\n","    critic_value_history = []  # Lưu lịch sử giá trị Critic\n","    rewards_history = []  # Lưu lịch sử phần thưởng\n","\n","    with tf.GradientTape() as tape:  # Bắt đầu ghi lại gradient\n","        for step in range(max_steps_per_episode):\n","            # Chuyển trạng thái thành tensor để đưa vào mô hình\n","            state = tf.convert_to_tensor(state, dtype=tf.float32)\n","            state = tf.expand_dims(state, axis=0)\n","\n","            # Lựa chọn hành động từ Actor và tính giá trị Critic\n","            action_probs, critic_value = model(state)\n","            action = np.random.choice(num_actions, p=np.squeeze(action_probs))  # Chọn hành động ngẫu nhiên dựa trên xác suất\n","\n","            action_probs_history.append(tf.math.log(action_probs[0, action]))  # Lưu log xác suất hành động\n","            critic_value_history.append(critic_value[0, 0])  # Lưu giá trị Critic\n","\n","            # Thực hiện hành động và nhận phản hồi từ môi trường\n","            state, reward, done, _ = env.step(action)\n","            rewards_history.append(reward)  # Lưu phần thưởng\n","            episode_reward += reward  # Cộng dồn phần thưởng\n","\n","            if done:  # Kết thúc tập nếu trạng thái kết thúc\n","                break\n","\n","        # Thêm giá trị cuối cùng để tính GAE\n","        critic_value_history.append(0 if done else model(tf.expand_dims(state, axis=0))[1][0, 0])\n","\n","        # Tính Advantage và Returns\n","        advantages, returns = compute_gae_and_returns(rewards_history, critic_value_history, gamma, lambda_)\n","\n","        # Chuẩn hóa Advantage để giảm phương sai\n","        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n","\n","        # Tính toán mất mát Actor, Critic và Entropy\n","        actor_losses = [-log_prob * adv for log_prob, adv in zip(action_probs_history, advantages)]\n","        critic_losses = [huber_loss(tf.expand_dims(v, 0), tf.expand_dims(r, 0)) for v, r in zip(critic_value_history[:-1], returns)]\n","        entropy_loss = -entropy_beta * tf.reduce_mean(tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-10), axis=1))\n","\n","        # Tổng hợp các loại mất mát\n","        loss_value = tf.reduce_sum(actor_losses) + 0.5 * tf.reduce_sum(critic_losses) + entropy_loss\n","\n","        # Tính gradient và cập nhật trọng số mô hình\n","        grads = tape.gradient(loss_value, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    # Theo dõi phần thưởng trung bình\n","    running_reward = 0.1 * episode_reward + 0.9 * running_reward\n","\n","    # Điều chỉnh Entropy Beta để giảm độ nhiễu\n","    entropy_beta = max(0.001, initial_entropy_beta * (1 - running_reward / 500))\n","\n","    # In kết quả mỗi 10 tập\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Running Reward: {running_reward:.2f}\")\n","\n","    # Dừng huấn luyện nếu đạt ngưỡng phần thưởng trung bình\n","    if running_reward > 475:\n","        print(f\"Đạt kết quả sau {episode_count} tập!\")\n","        break\n","\n","# Kết thúc huấn luyện nếu đạt số tập tối đa\n","if episode_count == max_episodes:\n","    print(f\"Đã huấn luyện xong sau {max_episodes} tập.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlTQNtc9qwFF","executionInfo":{"status":"ok","timestamp":1733195715463,"user_tz":-420,"elapsed":3765273,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"d4abd984-1239-49a2-8a0b-fd8991a49dcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 10, Running Reward: 6.63\n","Episode: 20, Running Reward: 8.84\n","Episode: 30, Running Reward: 10.44\n","Episode: 40, Running Reward: 30.46\n","Episode: 50, Running Reward: 56.09\n","Episode: 60, Running Reward: 95.94\n","Episode: 70, Running Reward: 122.78\n","Episode: 80, Running Reward: 132.42\n","Episode: 90, Running Reward: 234.89\n","Episode: 100, Running Reward: 192.00\n","Episode: 110, Running Reward: 254.86\n","Episode: 120, Running Reward: 145.21\n","Episode: 130, Running Reward: 136.94\n","Episode: 140, Running Reward: 152.56\n","Episode: 150, Running Reward: 258.28\n","Episode: 160, Running Reward: 237.88\n","Episode: 170, Running Reward: 227.89\n","Episode: 180, Running Reward: 301.63\n","Episode: 190, Running Reward: 430.83\n","Episode: 200, Running Reward: 363.27\n","Episode: 210, Running Reward: 376.79\n","Episode: 220, Running Reward: 400.64\n","Episode: 230, Running Reward: 412.40\n","Episode: 240, Running Reward: 428.00\n","Episode: 250, Running Reward: 445.14\n","Episode: 260, Running Reward: 402.52\n","Episode: 270, Running Reward: 274.41\n","Episode: 280, Running Reward: 218.07\n","Episode: 290, Running Reward: 212.17\n","Episode: 300, Running Reward: 274.48\n","Episode: 310, Running Reward: 281.94\n","Episode: 320, Running Reward: 217.49\n","Episode: 330, Running Reward: 165.11\n","Episode: 340, Running Reward: 148.09\n","Episode: 350, Running Reward: 142.43\n","Episode: 360, Running Reward: 142.51\n","Episode: 370, Running Reward: 137.54\n","Episode: 380, Running Reward: 135.93\n","Episode: 390, Running Reward: 110.79\n","Episode: 400, Running Reward: 110.67\n","Episode: 410, Running Reward: 114.08\n","Episode: 420, Running Reward: 117.66\n","Episode: 430, Running Reward: 125.38\n","Episode: 440, Running Reward: 125.10\n","Episode: 450, Running Reward: 128.45\n","Episode: 460, Running Reward: 144.83\n","Episode: 470, Running Reward: 155.57\n","Episode: 480, Running Reward: 129.40\n","Episode: 490, Running Reward: 120.38\n","Episode: 500, Running Reward: 127.21\n","Episode: 510, Running Reward: 136.67\n","Episode: 520, Running Reward: 152.32\n","Episode: 530, Running Reward: 163.88\n","Episode: 540, Running Reward: 175.03\n","Episode: 550, Running Reward: 206.09\n","Episode: 560, Running Reward: 352.17\n","Episode: 570, Running Reward: 423.09\n","Episode: 580, Running Reward: 339.85\n","Episode: 590, Running Reward: 312.41\n","Episode: 600, Running Reward: 280.17\n","Episode: 610, Running Reward: 257.78\n","Episode: 620, Running Reward: 220.29\n","Episode: 630, Running Reward: 205.27\n","Episode: 640, Running Reward: 224.57\n","Episode: 650, Running Reward: 205.24\n","Episode: 660, Running Reward: 194.26\n","Episode: 670, Running Reward: 162.82\n","Episode: 680, Running Reward: 151.84\n","Episode: 690, Running Reward: 160.78\n","Episode: 700, Running Reward: 268.76\n","Episode: 710, Running Reward: 352.94\n","Episode: 720, Running Reward: 258.69\n","Episode: 730, Running Reward: 180.52\n","Episode: 740, Running Reward: 149.64\n","Episode: 750, Running Reward: 209.85\n","Episode: 760, Running Reward: 382.33\n","Episode: 770, Running Reward: 326.56\n","Episode: 780, Running Reward: 257.27\n","Episode: 790, Running Reward: 231.12\n","Episode: 800, Running Reward: 233.51\n","Episode: 810, Running Reward: 366.53\n","Episode: 820, Running Reward: 453.46\n","Đạt kết quả sau 826 tập!\n"]}]}]}