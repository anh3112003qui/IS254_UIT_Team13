{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mô tả các bước chính của thuật toán Actor-Critic với DPG\n","# 1. **Xây dựng mô hình Actor và Critic**:\n","#    - Actor: Dự đoán hành động tối ưu dựa trên trạng thái hiện tại.\n","#    - Critic: Đánh giá giá trị của hành động đó thông qua Q-value.\n","#    - Target Networks: Các bản sao của Actor và Critic được cập nhật chậm để ổn định quá trình huấn luyện.\n","# 2. **Replay Buffer**:\n","#    - Lưu trữ lịch sử các trạng thái, hành động, phần thưởng để huấn luyện mô hình theo batch.\n","# 3. **Chọn hành động và thám hiểm**:\n","#    - Actor dự đoán hành động.\n","#    - Thêm nhiễu Gaussian để khuyến khích thám hiểm các hành động khác nhau.\n","# 4. **Huấn luyện Critic**:\n","#    - Sử dụng Replay Buffer để lấy dữ liệu.\n","#    - Tính toán Q-value mục tiêu từ mạng Target Critic.\n","#    - Cập nhật mạng Critic để dự đoán Q-value chính xác hơn.\n","# 5. **Huấn luyện Actor**:\n","#    - Dựa vào Critic để tìm hướng dẫn Actor chọn hành động tối ưu.\n","#    - Cập nhật mạng Actor.\n","# 6. **Cập nhật Target Networks**:\n","#    - Mạng Target Actor và Target Critic được cập nhật dần dần để theo kịp các thay đổi của Actor và Critic chính.\n","# 7. **Theo dõi hiệu suất**:\n","#    - Theo dõi phần thưởng trung bình động để đánh giá tiến trình.\n","#    - Giảm độ nhiễu để tăng khả năng khai thác (exploitation) khi mô hình đã ổn định.\n","# 8. **Dừng huấn luyện**:\n","#    - Dừng lại khi phần thưởng trung bình đạt đến ngưỡng mục tiêu hoặc khi đạt đến số lượng vòng lặp tối đa."],"metadata":{"id":"7iBSMPpKKirQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWzYuLoIsVbs"},"outputs":[],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","import gym\n","import numpy as np\n","import keras\n","from keras import ops\n","from keras import layers\n","import tensorflow as tf"]},{"cell_type":"code","source":["# ====================== Thiết lập môi trường ===========================\n","env = gym.make(\"CartPole-v1\")\n","num_inputs = 4 # Số lượng đầu vào (trạng thái của môi trường\n","num_actions = 1 # Số lượng đầu ra (hành động được thực hiện)\n","num_hidden = 256  # Số lượng nút trong lớp ẩn của mô hình.\n","gamma = 0.99 # Hệ số chiết khấu để tính giá trị Q.\n","exploration_noise_std = 0.3  # Khởi đầu với nhiễu cao\n","min_exploration_noise_std = 0.05  # Giảm dần đến giá trị tối thiểu\n","noise_decay = 0.995  # Tốc độ giảm nhiễu\n","max_steps_per_episode = 500\n","max_episodes = 1000"],"metadata":{"id":"4g-yOknWsbaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====================== Xây dựng Actor và Critic =======================\n","# **Bước 1: Khởi tạo Actor**\n","# Actor dự đoán hành động dựa trên trạng thái đầu vào\n","actor_inputs = layers.Input(shape=(num_inputs,))\n","actor_hidden = layers.Dense(num_hidden, activation=\"relu\")(actor_inputs)\n","actor_hidden = layers.Dense(num_hidden, activation=\"relu\")(actor_hidden)\n","actor_output = layers.Dense(num_actions, activation=\"tanh\")(actor_hidden)\n","actor_model = tf.keras.Model(inputs=actor_inputs, outputs=actor_output)\n","\n","# **Bước 2: Khởi tạo Critic**\n","# Critic đánh giá giá trị Q (Q-value) từ trạng thái và hành động\n","critic_inputs = layers.Input(shape=(num_inputs + num_actions,))\n","critic_hidden = layers.Dense(num_hidden, activation=\"relu\")(critic_inputs)\n","critic_hidden = layers.Dense(num_hidden, activation=\"relu\")(critic_hidden)\n","q_value = layers.Dense(1)(critic_hidden)\n","critic_model = tf.keras.Model(inputs=critic_inputs, outputs=q_value)\n","\n","# **Bước 3: Tạo Target Networks**\n","# Target Networks sử dụng để ổn định quá trình huấn luyện bằng cách chậm cập nhật so với mạng chính\n","target_actor_model = tf.keras.models.clone_model(actor_model)\n","target_critic_model = tf.keras.models.clone_model(critic_model)\n","\n","# ====================== Trình tối ưu hóa =======================\n","# **Bước 4: Định nghĩa bộ tối ưu hóa**\n","# Sử dụng Adam optimizer cho Actor và Critic\n","actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n","critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"],"metadata":{"id":"qnb35qjsHrD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====================== Replay Buffer =======================\n","# **Bước 5: Khởi tạo Replay Buffer**\n","# Replay Buffer lưu trữ lịch sử trạng thái, hành động, phần thưởng để huấn luyện theo batch\n","class ReplayBuffer:\n","    def __init__(self, buffer_capacity=200000, batch_size=128):\n","        self.buffer_capacity = buffer_capacity\n","        self.batch_size = batch_size\n","        self.buffer_counter = 0\n","        self.state_buffer = np.zeros((buffer_capacity, num_inputs))\n","        self.action_buffer = np.zeros((buffer_capacity, num_actions))\n","        self.reward_buffer = np.zeros((buffer_capacity, 1))\n","        self.next_state_buffer = np.zeros((buffer_capacity, num_inputs))\n","        self.done_buffer = np.zeros((buffer_capacity, 1))\n","\n","    # Lưu trữ trải nghiệm (state, action, reward, next_state, done)\n","    def store(self, state, action, reward, next_state, done):\n","        index = self.buffer_counter % self.buffer_capacity\n","        self.state_buffer[index] = state\n","        self.action_buffer[index] = action\n","        self.reward_buffer[index] = reward\n","        self.next_state_buffer[index] = next_state\n","        self.done_buffer[index] = done\n","        self.buffer_counter += 1\n","\n","    # Lấy mẫu batch ngẫu nhiên từ Replay Buffer\n","    def sample(self):\n","        max_buffer = min(self.buffer_counter, self.buffer_capacity)\n","        batch_indices = np.random.choice(max_buffer, self.batch_size)\n","        return (\n","            self.state_buffer[batch_indices],\n","            self.action_buffer[batch_indices],\n","            self.reward_buffer[batch_indices],\n","            self.next_state_buffer[batch_indices],\n","            self.done_buffer[batch_indices],\n","        )\n","\n","buffer = ReplayBuffer()"],"metadata":{"id":"Tuy_l8OhsjLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====================== Vòng lặp huấn luyện ===========================\n","# **Bước 6: Vòng lặp huấn luyện Actor-Critic**\n","running_reward = 0\n","tau = 0.005  # Hệ số cập nhật target network\n","\n","for episode in range(max_episodes):\n","    # Đặt lại môi trường và trạng thái ban đầu\n","    state = env.reset()\n","    episode_reward = 0\n","\n","    for step in range(max_steps_per_episode):\n","        # Chuyển trạng thái sang tensor\n","        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n","        state_tensor = tf.expand_dims(state_tensor, axis=0)\n","\n","        # **Actor chọn hành động**\n","        # Thêm nhiễu Gaussian để khuyến khích thám hiểm\n","        action = actor_model(state_tensor).numpy()[0]\n","        action += np.random.normal(0, exploration_noise_std)\n","        action = np.clip(action, -1, 1)\n","\n","        # **Thực hiện hành động và lưu trữ trải nghiệm**\n","        next_state, reward, done, _ = env.step(int(action > 0))\n","        buffer.store(state, action, reward, next_state, done)\n","        episode_reward += reward\n","\n","        # **Huấn luyện Actor và Critic khi Replay Buffer đủ dữ liệu**\n","        if buffer.buffer_counter >= buffer.batch_size:\n","            states, actions, rewards, next_states, dones = buffer.sample()\n","            states = tf.convert_to_tensor(states, dtype=tf.float32)\n","            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n","            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n","            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n","            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n","\n","            # **Huấn luyện Critic**\n","            with tf.GradientTape() as tape:\n","                # Tính Q-value mục tiêu từ Target Networks\n","                target_actions = target_actor_model(next_states)\n","                target_q_values = rewards + (1 - dones) * gamma * target_critic_model(\n","                    tf.concat([next_states, target_actions], axis=1)\n","                )\n","                # Tính Q-value dự đoán từ Critic model\n","                predicted_q_values = critic_model(tf.concat([states, actions], axis=1))\n","                critic_loss = tf.keras.losses.MSE(target_q_values, predicted_q_values)\n","\n","            # Cập nhật trọng số Critic\n","            critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n","            critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n","\n","            # **Huấn luyện Actor**\n","            with tf.GradientTape() as tape:\n","                actions_pred = actor_model(states)\n","                actor_loss = -tf.reduce_mean(critic_model(tf.concat([states, actions_pred], axis=1)))\n","\n","            # Cập nhật trọng số Actor\n","            actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n","            actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n","\n","            # **Cập nhật Target Networks**\n","            for target_param, param in zip(target_critic_model.trainable_variables, critic_model.trainable_variables):\n","                target_param.assign(tau * param + (1 - tau) * target_param)\n","\n","            for target_param, param in zip(target_actor_model.trainable_variables, actor_model.trainable_variables):\n","                target_param.assign(tau * param + (1 - tau) * target_param)\n","\n","        # Kiểm tra nếu trạng thái cuối cùng đạt đến điểm dừng\n","        if done:\n","            break\n","\n","        # Cập nhật trạng thái\n","        state = next_state\n","\n","    # **Giảm độ nhiễu theo thời gian để khuyến khích khai thác**\n","    exploration_noise_std = max(min_exploration_noise_std, exploration_noise_std * noise_decay)\n","\n","    # **Theo dõi phần thưởng trung bình động**\n","    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n","\n","    # In log sau mỗi 10 episode\n","    if episode % 10 == 0:\n","        print(f\"Episode: {episode}, Running Reward: {running_reward:.2f}, Exploration Noise: {exploration_noise_std:.3f}\")\n","\n","    # **Dừng khi đạt mục tiêu phần thưởng**\n","    if running_reward > 475:\n","        print(f\"Solved at episode {episode}!\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyhGUiM2IBsx","executionInfo":{"status":"ok","timestamp":1733163421139,"user_tz":-420,"elapsed":7776926,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"ec072898-4a83-4a31-baca-3ffc9489147d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-266f553b5683>:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  next_state, reward, done, _ = env.step(int(action > 0))\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 0, Running Reward: 0.85, Exploration Noise: 0.298\n","Episode: 10, Running Reward: 7.09, Exploration Noise: 0.284\n","Episode: 20, Running Reward: 8.09, Exploration Noise: 0.270\n","Episode: 30, Running Reward: 8.65, Exploration Noise: 0.257\n","Episode: 40, Running Reward: 8.96, Exploration Noise: 0.244\n","Episode: 50, Running Reward: 9.10, Exploration Noise: 0.232\n","Episode: 60, Running Reward: 9.22, Exploration Noise: 0.221\n","Episode: 70, Running Reward: 9.32, Exploration Noise: 0.210\n","Episode: 80, Running Reward: 9.06, Exploration Noise: 0.200\n","Episode: 90, Running Reward: 9.10, Exploration Noise: 0.190\n","Episode: 100, Running Reward: 9.06, Exploration Noise: 0.181\n","Episode: 110, Running Reward: 9.17, Exploration Noise: 0.172\n","Episode: 120, Running Reward: 9.15, Exploration Noise: 0.164\n","Episode: 130, Running Reward: 9.17, Exploration Noise: 0.156\n","Episode: 140, Running Reward: 9.14, Exploration Noise: 0.148\n","Episode: 150, Running Reward: 9.32, Exploration Noise: 0.141\n","Episode: 160, Running Reward: 9.36, Exploration Noise: 0.134\n","Episode: 170, Running Reward: 9.39, Exploration Noise: 0.127\n","Episode: 180, Running Reward: 9.38, Exploration Noise: 0.121\n","Episode: 190, Running Reward: 9.37, Exploration Noise: 0.115\n","Episode: 200, Running Reward: 9.36, Exploration Noise: 0.110\n","Episode: 210, Running Reward: 9.35, Exploration Noise: 0.104\n","Episode: 220, Running Reward: 9.35, Exploration Noise: 0.099\n","Episode: 230, Running Reward: 9.31, Exploration Noise: 0.094\n","Episode: 240, Running Reward: 9.27, Exploration Noise: 0.090\n","Episode: 250, Running Reward: 9.36, Exploration Noise: 0.085\n","Episode: 260, Running Reward: 9.38, Exploration Noise: 0.081\n","Episode: 270, Running Reward: 9.53, Exploration Noise: 0.077\n","Episode: 280, Running Reward: 9.55, Exploration Noise: 0.073\n","Episode: 290, Running Reward: 9.51, Exploration Noise: 0.070\n","Episode: 300, Running Reward: 9.46, Exploration Noise: 0.066\n","Episode: 310, Running Reward: 9.45, Exploration Noise: 0.063\n","Episode: 320, Running Reward: 9.60, Exploration Noise: 0.060\n","Episode: 330, Running Reward: 9.55, Exploration Noise: 0.057\n","Episode: 340, Running Reward: 9.40, Exploration Noise: 0.054\n","Episode: 350, Running Reward: 11.06, Exploration Noise: 0.052\n","Episode: 360, Running Reward: 67.81, Exploration Noise: 0.050\n","Episode: 370, Running Reward: 114.53, Exploration Noise: 0.050\n","Episode: 380, Running Reward: 150.25, Exploration Noise: 0.050\n","Episode: 390, Running Reward: 170.08, Exploration Noise: 0.050\n","Episode: 400, Running Reward: 251.64, Exploration Noise: 0.050\n","Episode: 410, Running Reward: 325.34, Exploration Noise: 0.050\n","Episode: 420, Running Reward: 318.25, Exploration Noise: 0.050\n","Episode: 430, Running Reward: 247.25, Exploration Noise: 0.050\n","Episode: 440, Running Reward: 202.79, Exploration Noise: 0.050\n","Episode: 450, Running Reward: 187.44, Exploration Noise: 0.050\n","Episode: 460, Running Reward: 204.92, Exploration Noise: 0.050\n","Episode: 470, Running Reward: 185.47, Exploration Noise: 0.050\n","Episode: 480, Running Reward: 173.99, Exploration Noise: 0.050\n","Episode: 490, Running Reward: 180.70, Exploration Noise: 0.050\n","Episode: 500, Running Reward: 176.59, Exploration Noise: 0.050\n","Episode: 510, Running Reward: 166.37, Exploration Noise: 0.050\n","Episode: 520, Running Reward: 158.14, Exploration Noise: 0.050\n","Episode: 530, Running Reward: 165.27, Exploration Noise: 0.050\n","Episode: 540, Running Reward: 246.57, Exploration Noise: 0.050\n","Episode: 550, Running Reward: 303.78, Exploration Noise: 0.050\n","Episode: 560, Running Reward: 379.46, Exploration Noise: 0.050\n","Episode: 570, Running Reward: 418.59, Exploration Noise: 0.050\n","Episode: 580, Running Reward: 451.26, Exploration Noise: 0.050\n","Episode: 590, Running Reward: 470.82, Exploration Noise: 0.050\n","Solved at episode 594!\n"]}]},{"cell_type":"markdown","source":["Exploration Noise trong code trên được sử dụng để khuyến khích Agent thử nghiệm các hành động đa dạng hơn, giúp khám phá môi trường hiệu quả hơn trong giai đoạn đầu huấn luyện. Nó giúp tránh việc Agent bị mắc kẹt trong các chiến lược không tối ưu do chưa đủ thông tin về môi trường. Khi huấn luyện tiến triển, độ nhiễu giảm dần để chuyển từ thám hiểm (exploration) sang khai thác (exploitation), tập trung vào các hành động tối ưu đã học được. Điều này đảm bảo sự cân bằng giữa việc khám phá và khai thác, cải thiện hiệu quả học tập của mô hình.\n","\n"],"metadata":{"id":"LSKmCIBcLQBo"}},{"cell_type":"code","source":[],"metadata":{"id":"7B99LhQyH5UN"},"execution_count":null,"outputs":[]}]}