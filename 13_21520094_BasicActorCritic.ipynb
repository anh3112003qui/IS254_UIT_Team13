{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO9xeuJhJuaC6O+NTZ6wBHx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Các bước chính thực hiện thuật toán Actor Critic trong code:**\n","# 1. Khởi tạo môi trường và các tham số cần thiết.\n","# 2. Trong mỗi episode, Actor chọn hành động dựa trên xác suất, Critic đánh giá hành động thông qua giá trị trạng thái.\n","# 3. Lưu trữ phần thưởng và thông tin cần thiết trong mỗi bước.\n","# 4. Khi episode kết thúc, tính giá trị hồi quy (returns) từ phần thưởng nhận được.\n","# 5. Tính toán lợi thế (advantage) để cải thiện Actor, giảm mất mát của Critic.\n","# 6. Cập nhật mô hình (Actor và Critic) bằng cách sử dụng gradient descent.\n","# 7. Lặp lại các bước trên cho đến khi đạt điều kiện dừng."],"metadata":{"id":"Lz4w5qS4B43r"}},{"cell_type":"code","source":["# Thêm thư viện\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # Thiết lập backend sử dụng TensorFlow\n","import gym\n","import numpy as np\n","import keras\n","from keras import ops\n","from keras import layers\n","import tensorflow as tf"],"metadata":{"id":"pg_IfH8de356","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733137554735,"user_tz":-420,"elapsed":4037,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"9b076be0-eaba-42b4-f546-4bfb013dad97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n","  from jax import xla_computation as _xla_computation\n"]}]},{"cell_type":"code","source":["# **Cấu hình tham số**\n","seed = 42  # Hạt giống ngẫu nhiên để tái lập kết quả\n","gamma = 0.99  # Hệ số giảm giá (discount factor) cho giá trị hồi quy\n","max_steps_per_episode = 500  # Số bước tối đa trong một episode\n","env = gym.make(\"CartPole-v1\")  # Tạo môi trường CartPole\n","env.seed(seed)  # Đặt hạt giống cho môi trường\n","eps = np.finfo(np.float32).eps.item()  # Một giá trị epsilon nhỏ để tránh lỗi chia cho 0\n","\n","# **Xác định kiến trúc mạng**\n","num_inputs = 4  # Số lượng đầu vào từ môi trường (4 trạng thái: vị trí, góc, vận tốc, tốc độ góc)\n","num_actions = 2  # Số lượng hành động (trái hoặc phải)\n","num_hidden = 128  # Số lượng nơ-ron trong lớp ẩn"],"metadata":{"id":"ZlmdQFNUizB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733137558172,"user_tz":-420,"elapsed":528,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"792ee359-be4e-41a4-d26a-70a3e4f7e816"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["# **Xây dựng mạng Actor-Critic**\n","inputs = layers.Input(shape=(num_inputs,))  # Định nghĩa đầu vào (state)\n","common = layers.Dense(num_hidden, activation=\"relu\")(inputs)  # Lớp ẩn chung với 128 nơ-ron\n","action = layers.Dense(num_actions, activation=\"softmax\")(common)  # Lớp Actor dự đoán xác suất các hành động\n","critic = layers.Dense(1)(common)  # Lớp Critic dự đoán giá trị trạng thái\n","\n","model = keras.Model(inputs=inputs, outputs=[action, critic])  # Kết hợp Actor và Critic thành một mô hình\n","\n","# **Tối ưu hóa và hàm mất mát**\n","optimizer = keras.optimizers.Adam(learning_rate=0.01)  # Trình tối ưu hóa Adam với tốc độ học 0.01\n","huber_loss = keras.losses.Huber()  # Hàm mất mát Huber (giảm ảnh hưởng của outliers)"],"metadata":{"id":"G59DW0vqi1aD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **Biến lưu lịch sử và thiết lập ban đầu**\n","action_probs_history = []  # Lưu log xác suất hành động đã chọn\n","critic_value_history = []  # Lưu giá trị Critic\n","rewards_history = []  # Lưu phần thưởng trong tập\n","running_reward = 0  # Phần thưởng trung bình động\n","episode_count = 0  # Đếm số episode đã hoàn thành\n","max_episodes = 1000  # Giới hạn số episode để huấn luyện"],"metadata":{"id":"RuyieCHCCU7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **Vòng lặp huấn luyện**\n","while episode_count < max_episodes:\n","    state = env.reset()  # Reset môi trường, lấy trạng thái ban đầu\n","    episode_reward = 0  # Tổng phần thưởng của episode\n","\n","    with tf.GradientTape() as tape:  # Bắt đầu theo dõi các phép tính để tính gradient\n","        for timestep in range(1, max_steps_per_episode + 1):\n","            state = ops.convert_to_tensor(state)  # Chuyển trạng thái thành tensor\n","            state = ops.expand_dims(state, 0)  # Thêm chiều batch\n","\n","            action_probs, critic_value = model(state)  # Mô hình dự đoán xác suất hành động và giá trị Critic\n","            critic_value_history.append(critic_value[0, 0])  # Lưu giá trị Critic\n","\n","            action = np.random.choice(num_actions, p=np.squeeze(action_probs))  # Chọn hành động dựa trên xác suất\n","            action_probs_history.append(ops.log(action_probs[0, action]))  # Lưu log xác suất của hành động đã chọn\n","\n","            state, reward, done, _ = env.step(action)  # Áp dụng hành động trong môi trường\n","            rewards_history.append(reward)  # Lưu phần thưởng\n","            episode_reward += reward  # Cộng dồn phần thưởng\n","\n","            if done:  # Nếu môi trường báo kết thúc\n","                break\n","\n","        # **Tính giá trị hồi quy (returns)**\n","        returns = []  # Danh sách lưu giá trị hồi quy\n","        discounted_sum = 0  # Khởi tạo giá trị hồi quy\n","        for r in rewards_history[::-1]:  # Duyệt ngược danh sách phần thưởng\n","            discounted_sum = r + gamma * discounted_sum  # Áp dụng công thức giảm giá\n","            returns.insert(0, discounted_sum)  # Lưu giá trị hồi quy\n","\n","        returns = np.array(returns)  # Chuyển sang numpy array\n","        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)  # Chuẩn hóa giá trị\n","        returns = returns.tolist()  # Chuyển lại thành danh sách\n","\n","        # **Tính toán mất mát**\n","        history = zip(action_probs_history, critic_value_history, returns)  # Gộp các giá trị để tính toán\n","        actor_losses = []  # Lưu mất mát của Actor\n","        critic_losses = []  # Lưu mất mát của Critic\n","        for log_prob, value, ret in history:\n","            advantage = ret - value  # Tính lợi thế (advantage)\n","            actor_losses.append(-log_prob * advantage)  # Mất mát Actor dựa trên lợi thế\n","            critic_losses.append(huber_loss(ops.expand_dims(value, 0), ops.expand_dims(ret, 0)))  # Mất mát Critic\n","\n","        loss_value = sum(actor_losses) + sum(critic_losses)  # Tổng mất mát\n","\n","        grads = tape.gradient(loss_value, model.trainable_variables)  # Tính gradient\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Cập nhật trọng số\n","\n","        action_probs_history.clear()  # Xóa lịch sử hành động\n","        critic_value_history.clear()  # Xóa lịch sử giá trị Critic\n","        rewards_history.clear()  # Xóa lịch sử phần thưởng\n","\n","    episode_count += 1  # Tăng số episode\n","    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward  # Cập nhật phần thưởng trung bình động\n","\n","    if episode_count % 10 == 0:  # Log mỗi 10 episode\n","        template = \"Episode: {}, Running Reward: {:.2f}\"\n","        print(template.format(episode_count, running_reward))\n","\n","    if running_reward > 475:  # Điều kiện dừng khi đạt phần thưởng mục tiêu\n","        print(\"Solved at episode {}!\".format(episode_count))\n","        break\n","\n","if episode_count == max_episodes:  # Nếu đạt giới hạn số episode\n","    print(f\"Reached the maximum episode limit of {max_episodes}. Training stopped.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxKLl_TVi5LP","executionInfo":{"status":"ok","timestamp":1733139334241,"user_tz":-420,"elapsed":1764644,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"bbd298da-c7a5-452f-b8a9-bcd4d084d571"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 10, Running Reward: 15.18\n","Episode: 20, Running Reward: 25.57\n","Episode: 30, Running Reward: 23.03\n","Episode: 40, Running Reward: 19.60\n","Episode: 50, Running Reward: 16.89\n","Episode: 60, Running Reward: 17.77\n","Episode: 70, Running Reward: 21.20\n","Episode: 80, Running Reward: 37.51\n","Episode: 90, Running Reward: 39.03\n","Episode: 100, Running Reward: 72.78\n","Episode: 110, Running Reward: 57.92\n","Episode: 120, Running Reward: 43.30\n","Episode: 130, Running Reward: 37.61\n","Episode: 140, Running Reward: 57.23\n","Episode: 150, Running Reward: 83.04\n","Episode: 160, Running Reward: 114.87\n","Episode: 170, Running Reward: 90.36\n","Episode: 180, Running Reward: 69.34\n","Episode: 190, Running Reward: 55.32\n","Episode: 200, Running Reward: 52.66\n","Episode: 210, Running Reward: 50.68\n","Episode: 220, Running Reward: 63.15\n","Episode: 230, Running Reward: 111.07\n","Episode: 240, Running Reward: 118.11\n","Episode: 250, Running Reward: 119.31\n","Episode: 260, Running Reward: 120.00\n","Episode: 270, Running Reward: 120.26\n","Episode: 280, Running Reward: 126.27\n","Episode: 290, Running Reward: 151.96\n","Episode: 300, Running Reward: 244.53\n","Episode: 310, Running Reward: 284.89\n","Episode: 320, Running Reward: 246.38\n","Episode: 330, Running Reward: 200.72\n","Episode: 340, Running Reward: 175.60\n","Episode: 350, Running Reward: 158.94\n","Episode: 360, Running Reward: 186.00\n","Episode: 370, Running Reward: 258.78\n","Episode: 380, Running Reward: 194.23\n","Episode: 390, Running Reward: 134.56\n","Episode: 400, Running Reward: 97.38\n","Episode: 410, Running Reward: 97.77\n","Episode: 420, Running Reward: 93.25\n","Episode: 430, Running Reward: 98.02\n","Episode: 440, Running Reward: 109.58\n","Episode: 450, Running Reward: 116.92\n","Episode: 460, Running Reward: 120.35\n","Episode: 470, Running Reward: 129.22\n","Episode: 480, Running Reward: 146.22\n","Episode: 490, Running Reward: 155.68\n","Episode: 500, Running Reward: 192.19\n","Episode: 510, Running Reward: 193.88\n","Episode: 520, Running Reward: 174.31\n","Episode: 530, Running Reward: 159.11\n","Episode: 540, Running Reward: 139.21\n","Episode: 550, Running Reward: 132.14\n","Episode: 560, Running Reward: 130.51\n","Episode: 570, Running Reward: 133.31\n","Episode: 580, Running Reward: 135.49\n","Episode: 590, Running Reward: 137.21\n","Episode: 600, Running Reward: 141.39\n","Episode: 610, Running Reward: 155.32\n","Episode: 620, Running Reward: 182.75\n","Episode: 630, Running Reward: 225.77\n","Episode: 640, Running Reward: 335.81\n","Episode: 650, Running Reward: 381.34\n","Episode: 660, Running Reward: 268.63\n","Episode: 670, Running Reward: 217.25\n","Episode: 680, Running Reward: 192.74\n","Episode: 690, Running Reward: 225.66\n","Episode: 700, Running Reward: 223.90\n","Episode: 710, Running Reward: 303.72\n","Episode: 720, Running Reward: 354.11\n","Episode: 730, Running Reward: 396.27\n","Episode: 740, Running Reward: 437.89\n","Episode: 750, Running Reward: 414.43\n","Episode: 760, Running Reward: 446.37\n","Episode: 770, Running Reward: 467.89\n","Solved at episode 775!\n"]}]},{"cell_type":"markdown","source":["**Giải thích thêm:**\n","- Actor dự đoán hành động tối ưu dựa trên xác suất.\n","- Critic giúp đánh giá hành động bằng cách tính giá trị trạng thái.\n","- Gradient loss từ cả Actor và Critic giúp cập nhật mạng để cải thiện hiệu suất qua từng episode."],"metadata":{"id":"mAj0WN_mCk13"}}]}