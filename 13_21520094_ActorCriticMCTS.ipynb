{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9WOsKvK/icgx2vJU4MstO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MÔ TẢ CÁC BƯỚC TRONG ACTOR-CRITIC VỚI MCTS\n","# Actor-Critic kết hợp Monte Carlo Tree Search (MCTS) là một cách tiếp cận kết hợp giữa học tăng cường và tìm kiếm cây. Trong thuật toán này:\n","# 1. **Actor (Chính sách)**: Xác định xác suất chọn các hành động từ trạng thái.\n","# 2. **Critic (Giá trị)**: Ước lượng giá trị trạng thái để hỗ trợ Actor điều chỉnh chính sách.\n","# 3. **MCTS**: Mô phỏng các hành động tiềm năng từ trạng thái hiện tại, giúp lựa chọn hành động tối ưu bằng cách sử dụng thông tin của Actor và Critic.\n","# 4. **Entropy Regularization**: Tăng độ đa dạng trong các hành động được chọn để khuyến khích khám phá.\n","# **Các bước chính trong thuật toán:**\n","# 1. **Khởi tạo**: Tạo môi trường, thiết lập mô hình Actor-Critic và các thông số.\n","# 2. **MCTS**: Mô phỏng nhiều lần để xác định hành động tốt nhất tại mỗi trạng thái.\n","# 3. **Thu thập dữ liệu**: Chạy tập huấn luyện, ghi nhận phần thưởng và hành động.\n","# 4. **Tính Advantage và Returns**: Sử dụng giá trị Critic và phần thưởng để tính lợi thế.\n","# 5. **Cập nhật mô hình**: Tối ưu hóa Actor và Critic dựa trên mất mát.\n","# 6. **Theo dõi kết quả**: Lưu lại phần thưởng trung bình để đánh giá hiệu suất.\n"],"metadata":{"id":"Xb7SZ-UKwrOF"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EiQthBX7rjwq","executionInfo":{"status":"ok","timestamp":1733196753492,"user_tz":-420,"elapsed":5497,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"07df9e39-96f8-4582-8ef9-181c1b95d5e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n","  from jax import xla_computation as _xla_computation\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","import gym\n","import numpy as np\n","import keras\n","from keras import ops\n","from keras import layers\n","import tensorflow as tf"]},{"cell_type":"code","source":["# ==================== CẤU HÌNH THUẬT TOÁN ====================\n","# Khởi tạo các tham số chính để thực hiện Actor-Critic kết hợp với MCTS\n","seed = 42  # Seed để tái hiện kết quả\n","gamma = 0.99  # Hệ số chiết khấu, quyết định tầm quan trọng của phần thưởng tương lai\n","max_steps_per_episode = 1000  # Số bước tối đa cho mỗi tập huấn luyện\n","env = gym.make(\"CartPole-v1\")  # Tạo môi trường CartPole\n","env.seed(seed)  # Gán seed cho môi trường\n","eps = np.finfo(np.float32).eps.item()  # Giá trị epsilon nhỏ để tránh chia cho 0 trong tính toán\n","\n","# ==================== XÂY DỰNG MÔ HÌNH ACTOR-CRITIC ====================\n","# Thiết lập cấu trúc mô hình neural mạng cho Actor và Critic\n","num_inputs = 4  # Số lượng đầu vào (tính năng trạng thái của môi trường)\n","num_actions = 2  # Số lượng hành động có thể thực hiện\n","num_hidden = 256  # Số lượng neuron trong tầng ẩn"],"metadata":{"id":"s7zhg-jtrylY","executionInfo":{"status":"ok","timestamp":1733196753492,"user_tz":-420,"elapsed":5,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a205d462-a55d-40ac-88d5-c4c3df43c6c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["# Xây dựng mô hình Actor-Critic\n","# Tạo đầu vào của mạng\n","inputs = layers.Input(shape=(num_inputs,))\n","# Tầng ẩn 1 với kích hoạt ReLU\n","common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n","# Tầng ẩn 2 với kích hoạt ReLU\n","common = layers.Dense(num_hidden, activation=\"relu\")(common)\n","# Tầng đầu ra cho Actor: trả về xác suất các hành động\n","action = layers.Dense(num_actions, activation=\"softmax\")(common)\n","# Tầng đầu ra cho Critic: trả về giá trị trạng thái\n","critic = layers.Dense(1)(common)\n","# Kết hợp các tầng thành một mô hình\n","model = keras.Model(inputs=inputs, outputs=[action, critic])\n","\n","# ==================== TỐI ƯU HÓA MÔ HÌNH ====================\n","# Sử dụng Adam optimizer với learning rate\n","optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","# Sử dụng hàm mất mát Huber để giảm ảnh hưởng của ngoại lệ trong Critic\n","huber_loss = keras.losses.Huber()\n","\n","# ==================== BIẾN LƯU TRỮ LỊCH SỬ ====================\n","# Lưu trữ lịch sử xác suất hành động, giá trị Critic và phần thưởng\n","action_probs_history = []  # Xác suất của các hành động đã thực hiện\n","critic_value_history = []  # Giá trị trạng thái từ Critic\n","rewards_history = []  # Phần thưởng nhận được\n","running_reward = 0  # Phần thưởng trung bình chạy qua các tập\n","episode_count = 0  # Bộ đếm số tập huấn luyện\n","max_episodes = 1000  # Số tập tối đa\n","entropy_beta = 0.05  # Hệ số cho entropy regularization\n","num_simulations = 50  # Số lần mô phỏng trong MCTS\n","running_rewards = []  # Lưu phần thưởng trung bình"],"metadata":{"id":"UxpSmUhir--x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mô hình hóa một nút trong cây tìm kiếm MCTS\n","class MCTSNode:\n","    def __init__(self, state, parent=None, prior_prob=1.0):\n","        self.state = state  # Trạng thái tương ứng với nút\n","        self.parent = parent  # Nút cha\n","        self.children = {}  # Danh sách các nút con\n","        self.visits = 0  # Số lần nút này được thăm\n","        self.value = 0  # Giá trị trung bình của nút\n","        self.prior_prob = prior_prob  # Xác suất ban đầu của hành động tương ứng\n","\n","    def is_fully_expanded(self):\n","        # Kiểm tra xem nút đã mở rộng tất cả các hành động hay chưa\n","        return len(self.children) == num_actions\n","\n","    def best_child(self, c_puct=1.0):\n","        # Tìm nút con tốt nhất dựa trên giá trị UCB (Upper Confidence Bound)\n","        return max(\n","            self.children.items(),\n","            key=lambda child: child[1].value / (1 + child[1].visits) +\n","                              c_puct * child[1].prior_prob * np.sqrt(self.visits) / (1 + child[1].visits)\n","        )[1]\n","\n","    def expand(self, action_probs):\n","        # Mở rộng nút bằng cách thêm các hành động mới chưa được thêm vào cây\n","        for action, prob in enumerate(action_probs):\n","            if action not in self.children:\n","                self.children[action] = MCTSNode(self.state, parent=self, prior_prob=prob)\n","\n","    def backpropagate(self, reward):\n","        # Lan truyền phần thưởng ngược từ nút hiện tại về gốc\n","        self.value += reward\n","        self.visits += 1\n","        if self.parent:\n","            self.parent.backpropagate(reward)\n"],"metadata":{"id":"URPkeDuVsBxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hàm thực hiện thuật toán Monte Carlo Tree Search\n","def run_mcts(root, model, num_simulations=num_simulations, c_puct=1.0):\n","    for _ in range(num_simulations):\n","        node = root  # Bắt đầu từ nút gốc\n","        # Duyệt cây cho đến khi gặp nút chưa mở rộng\n","        while node.is_fully_expanded() and node.children:\n","            node = node.best_child(c_puct)\n","        # Chuyển trạng thái nút thành tensor để dự đoán xác suất hành động và giá trị Critic\n","        state_tensor = tf.convert_to_tensor(node.state, dtype=tf.float32)\n","        state_tensor = tf.expand_dims(state_tensor, 0)\n","        action_probs, critic_value = model(state_tensor)\n","        action_probs = action_probs.numpy().squeeze()\n","        # Mở rộng cây nếu nút hiện tại chưa được mở rộng\n","        if not node.is_fully_expanded():\n","            node.expand(action_probs)\n","        # Nhận giá trị phần thưởng từ Critic\n","        reward = critic_value.numpy()[0, 0]\n","        # Lan truyền phần thưởng ngược lên cây\n","        node.backpropagate(reward)\n","    # Trả về hành động với lượt thăm cao nhất\n","    return max(root.children.items(), key=lambda child: child[1].visits)[0]\n","\n","\n","# Loss với Entropy Regularization\n","def compute_loss(action_probs_history, critic_value_history, returns, entropy_beta):\n","    actor_losses, critic_losses = [], []\n","    entropy = 0\n","    for log_prob, value, ret in zip(action_probs_history, critic_value_history, returns):\n","        advantage = ret - value\n","        actor_losses.append(-log_prob * advantage)\n","        critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n","        entropy += -tf.reduce_sum(action_probs_history[-1] * tf.math.log(action_probs_history[-1] + eps))\n","    total_loss = sum(actor_losses) + sum(critic_losses) - entropy_beta * entropy\n","    return total_loss"],"metadata":{"id":"kMb9hZEesG_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vòng huấn luyện Actor-Critic\n","# Huấn luyện mô hình Actor-Critic với MCTS\n","while episode_count < max_episodes:\n","    state = env.reset()  # Đặt lại môi trường\n","    episode_reward = 0  # Tổng phần thưởng trong tập\n","    with tf.GradientTape() as tape:  # Gradient Tape để theo dõi gradient\n","        for timestep in range(1, max_steps_per_episode + 1):\n","            # Chuyển trạng thái thành tensor\n","            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n","            state_tensor = tf.expand_dims(state_tensor, 0)\n","            action_probs, critic_value = model(state_tensor)  # Lấy dự đoán từ Actor và Critic\n","            critic_value_history.append(critic_value[0, 0])  # Lưu giá trị Critic\n","            root = MCTSNode(state)  # Tạo nút gốc MCTS với trạng thái hiện tại\n","            root.expand(action_probs.numpy().squeeze())  # Mở rộng nút gốc\n","            action = run_mcts(root, model)  # Chạy MCTS để chọn hành động\n","            action_probs_history.append(action_probs[0, action])  # Lưu xác suất hành động\n","            state, reward, done, _ = env.step(action)  # Thực hiện hành động\n","            rewards_history.append(reward)  # Lưu phần thưởng\n","            episode_reward += reward  # Cộng phần thưởng vào tổng tập\n","            if done:  # Dừng tập nếu trạng thái kết thúc\n","                break\n","\n","        # Tính toán Returns và Advantage\n","        returns = []\n","        discounted_sum = 0\n","        for r in rewards_history[::-1]:\n","            discounted_sum = r + gamma * discounted_sum\n","            returns.insert(0, discounted_sum)\n","        returns = np.array(returns)\n","        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)  # Chuẩn hóa Returns\n","        returns = returns.tolist()\n","\n","        # Tính mất mát và cập nhật mô hình\n","        loss_value = compute_loss(action_probs_history, critic_value_history, returns, entropy_beta)\n","        grads = tape.gradient(loss_value, model.trainable_variables)  # Lấy gradient\n","        grads, _ = tf.clip_by_global_norm(grads, 1.0)  # Cắt gradient để tránh quá lớn\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Cập nhật mô hình\n","\n","        # Dọn lịch sử sau mỗi tập\n","        action_probs_history.clear()\n","        critic_value_history.clear()\n","        rewards_history.clear()\n","\n","    episode_count += 1\n","    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n","    running_rewards.append(running_reward)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Running Reward: {running_reward:.2f}\")\n","    if running_reward > 475:\n","        print(f\"Solved at episode {episode_count}!\")\n","        break\n","\n","if episode_count == max_episodes:\n","    print(f\"Reached the maximum episode limit of {max_episodes}. Training stopped.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kO1X1yFsJhf","executionInfo":{"status":"ok","timestamp":1733199534969,"user_tz":-420,"elapsed":2751145,"user":{"displayName":"Lê Trần Anh Quí","userId":"10347017880043488182"}},"outputId":"f82eaec5-9976-4235-e9c8-afb4d202d671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 10, Running Reward: 3.75\n","Episode: 20, Running Reward: 6.10\n","Episode: 30, Running Reward: 7.60\n","Episode: 40, Running Reward: 8.27\n","Episode: 50, Running Reward: 8.69\n","Episode: 60, Running Reward: 8.98\n","Episode: 70, Running Reward: 9.22\n","Episode: 80, Running Reward: 9.30\n","Episode: 90, Running Reward: 9.24\n","Episode: 100, Running Reward: 9.40\n","Episode: 110, Running Reward: 9.56\n","Episode: 120, Running Reward: 9.42\n","Episode: 130, Running Reward: 9.47\n","Episode: 140, Running Reward: 9.53\n","Episode: 150, Running Reward: 9.44\n","Episode: 160, Running Reward: 9.32\n","Episode: 170, Running Reward: 9.41\n","Episode: 180, Running Reward: 9.30\n","Episode: 190, Running Reward: 9.49\n","Episode: 200, Running Reward: 9.60\n","Episode: 210, Running Reward: 9.53\n","Episode: 220, Running Reward: 9.42\n","Episode: 230, Running Reward: 9.41\n","Episode: 240, Running Reward: 9.31\n","Episode: 250, Running Reward: 9.20\n","Episode: 260, Running Reward: 9.21\n","Episode: 270, Running Reward: 9.23\n","Episode: 280, Running Reward: 9.23\n","Episode: 290, Running Reward: 9.11\n","Episode: 300, Running Reward: 9.28\n","Episode: 310, Running Reward: 9.30\n","Episode: 320, Running Reward: 9.32\n","Episode: 330, Running Reward: 9.34\n","Episode: 340, Running Reward: 9.28\n","Episode: 350, Running Reward: 9.15\n","Episode: 360, Running Reward: 9.07\n","Episode: 370, Running Reward: 9.08\n","Episode: 380, Running Reward: 9.26\n","Episode: 390, Running Reward: 9.40\n","Episode: 400, Running Reward: 9.44\n","Episode: 410, Running Reward: 9.27\n","Episode: 420, Running Reward: 9.26\n","Episode: 430, Running Reward: 9.25\n","Episode: 440, Running Reward: 9.15\n","Episode: 450, Running Reward: 9.16\n","Episode: 460, Running Reward: 9.24\n","Episode: 470, Running Reward: 9.16\n","Episode: 480, Running Reward: 9.49\n","Episode: 490, Running Reward: 9.36\n","Episode: 500, Running Reward: 9.37\n","Episode: 510, Running Reward: 9.55\n","Episode: 520, Running Reward: 9.54\n","Episode: 530, Running Reward: 9.45\n","Episode: 540, Running Reward: 9.46\n","Episode: 550, Running Reward: 9.48\n","Episode: 560, Running Reward: 9.35\n","Episode: 570, Running Reward: 9.27\n","Episode: 580, Running Reward: 9.20\n","Episode: 590, Running Reward: 9.14\n","Episode: 600, Running Reward: 9.27\n","Episode: 610, Running Reward: 9.28\n","Episode: 620, Running Reward: 9.41\n","Episode: 630, Running Reward: 9.21\n","Episode: 640, Running Reward: 9.26\n","Episode: 650, Running Reward: 9.38\n","Episode: 660, Running Reward: 9.28\n","Episode: 670, Running Reward: 9.21\n","Episode: 680, Running Reward: 9.29\n","Episode: 690, Running Reward: 9.39\n","Episode: 700, Running Reward: 9.34\n","Episode: 710, Running Reward: 9.52\n","Episode: 720, Running Reward: 9.59\n","Episode: 730, Running Reward: 9.48\n","Episode: 740, Running Reward: 9.50\n","Episode: 750, Running Reward: 9.40\n","Episode: 760, Running Reward: 9.42\n","Episode: 770, Running Reward: 9.37\n","Episode: 780, Running Reward: 9.39\n","Episode: 790, Running Reward: 9.31\n","Episode: 800, Running Reward: 9.43\n","Episode: 810, Running Reward: 9.55\n","Episode: 820, Running Reward: 9.42\n","Episode: 830, Running Reward: 9.53\n","Episode: 840, Running Reward: 9.49\n","Episode: 850, Running Reward: 9.30\n","Episode: 860, Running Reward: 9.44\n","Episode: 870, Running Reward: 9.41\n","Episode: 880, Running Reward: 9.27\n","Episode: 890, Running Reward: 9.34\n","Episode: 900, Running Reward: 9.32\n","Episode: 910, Running Reward: 9.27\n","Episode: 920, Running Reward: 9.36\n","Episode: 930, Running Reward: 9.29\n","Episode: 940, Running Reward: 9.23\n","Episode: 950, Running Reward: 9.15\n","Episode: 960, Running Reward: 9.25\n","Episode: 970, Running Reward: 9.28\n","Episode: 980, Running Reward: 9.18\n","Episode: 990, Running Reward: 9.36\n","Episode: 1000, Running Reward: 9.34\n","Reached the maximum episode limit of 1000. Training stopped.\n"]}]}]}